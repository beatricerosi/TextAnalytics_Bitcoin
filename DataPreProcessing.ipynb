{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprensione, analisi e pulizia dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il file originale da oltre 16M di tweets è stato diviso in dataset più piccoli da 1M di righe circa ciascuno (motivi computazionali) per ognuna delle operazioni di pre-processing dei dati, ragione per cui vengono caricati dataset con nomi diversi, in genere contenenti un numero, e per cui c'è molto codice ridondante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfc = pd.read_csv('.../en_tweets.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "dfc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbiamo selezionato soltanto i tweets in lingua inglese\n",
    "#su porzioni del dataset originale\n",
    "from langdetect import detect\n",
    "prova1=dfc[15000001:]\n",
    "\n",
    "def det(x):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'Other'\n",
    "    return lang\n",
    "prova1['lang']= prova1['text'].apply(det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciascuna di queste parti è stata salvata in un file diverso\n",
    "prova1.to_csv('.../split15.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtraggio dei tweet in base alla lingua\n",
    "#abbiamo considerato solo tweet in inglese\n",
    "en1=prova1[prova1['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#di nuovo salvataggio del file\n",
    "en1.to_csv('.../en15.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dopo aver ripetuto il codice una seconda volta,\n",
    "#man mano abbiamo unito tutti i file fino ad ottenere un dataset unico\n",
    "en1_2=pd.concat([en1, en2], axis=0)\n",
    "en1_2.to_csv('.../sa1_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS CON VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d= pd.read_csv('.../en_tweets.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbiamo fatto la sentiment analysis su tutto il dataset \n",
    "#applicando il codice a 1M di tweets alla volta\n",
    "\n",
    "#Sono state aggiunte al dataset 4 colonne: 'compound', 'pos', 'neu', 'neg'\n",
    "analyzer= SentimentIntensityAnalyzer()\n",
    "d1=d[1000001:2000000]\n",
    "for row in d1:\n",
    "    d1['compound']=[analyzer.polarity_scores(x)['compound'] for x in d1['text']]\n",
    "    d1['neg']=[analyzer.polarity_scores(x)['neg'] for x in d1['text']]\n",
    "    d1['neu']=[analyzer.polarity_scores(x)['neu'] for x in d1['text']]\n",
    "    d1['pos']=[analyzer.polarity_scores(x)['pos'] for x in d1['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciascuna di queste parti è stata salvata in un file diverso\n",
    "d1.to_csv('.../sa1.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenazione dei file\n",
    "# d2=d[2000001:3000000]\n",
    "d1_2=pd.concat([d1, d2], axis=0)\n",
    "d1_2.to_csv('.../sa1_2.csv', index = False) #salvataggio finale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop delle colonne superflue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.../sa4.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1 = df.drop(columns= ['replies', 'likes', 'retweets', 'lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1.to_csv('.../drop4.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colonna 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "prova = pd.read_csv('.../drop4.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "prova['date'] = pd.to_datetime(prova['date'])\n",
    "prova = prova.set_index(prova['date'])\n",
    "prova = prova.sort_index()\n",
    "cut = prova['2019-03-25':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut.to_csv('.../drop1.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprensione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.../df2019.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizzazione dei dati \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(df['compound'], hist=True, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['date', 'user','timestamp', 'text', 'compound', 'neg', 'neu', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[df.pos > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~df['text'].str.contains('BitCoin|bitcoin|BTC|btc|Bitcoin|BITCOIN|COIN|coin|Crypto|crypto|Btc')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df[df['text'].str.contains('BitCoin|bitcoin|BTC|btc|Bitcoin|BITCOIN|COIN|coin|Crypto|crypto|CRYPTO|Btc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['compound'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulizia del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import regex as re\n",
    "import contractions\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def process(text):\n",
    "    #Convertire a lower case\n",
    "    text = text.lower()\n",
    "    #Trasformare www.* oppure https?://* in URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "    #Rimuovere @username \n",
    "    text = re.sub('@[^\\s]+',' ',text)\n",
    "    #Rimuovere spazi bianchi in più\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    #Sostituire #parola con parola\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    #trim\n",
    "    text = text.strip('\\'\"')\n",
    "    #Rimozione della punteggiatura\n",
    "    for punctuation in string.punctuation: \n",
    "        text = text.replace(punctuation, ' ') \n",
    "    #tenere solo parole\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    #tokenizzare\n",
    "    words = nltk.tokenize.casual_tokenize(text)\n",
    "    #Rimuovere stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    clean = [w for w in words if not w in stops]\n",
    "    return ' '.join(clean)\n",
    "\n",
    "df['clean'] = df['text'].apply(lambda text: process(text))\n",
    "df.sample(3, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbiamo eliminato la sentiment analysis fatta in precedenza per ripeterla sul testo pulito \n",
    "#ed ottenere una maggiore precisione\n",
    "new = df.drop(columns= ['text', 'compound', 'neu', 'neg', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer= SentimentIntensityAnalyzer()\n",
    "for row in new:\n",
    "    new['neg']=[analyzer.polarity_scores(x)['neg'] for x in new['clean']]\n",
    "    new['neu']=[analyzer.polarity_scores(x)['neu'] for x in new['clean']]\n",
    "    new['pos']=[analyzer.polarity_scores(x)['pos'] for x in new['clean']]\n",
    "    new['compound']=[analyzer.polarity_scores(x)['compound'] for x in new['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.to_csv('.../file7.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eravamo consapevoli della necessità della pulizia del testo per la SA, ma il nostro intento era cercare di avere più valori possibili da comparare. Infatti abbiamo tentato anche di dare un'etichetta unica per ciascun tweet (pos, neg e neu) invece di un valore compreso tra 0 e 1 per \"positive, negative e neutral\". Come ci aspettavamo, la scelta dell'eticehtta unica era troppo approssimativa e spesso errata, al contrario dei singoli valori dopo la pulizia del testo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotazione dei tweets con 'bull' oppure 'bear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.../file10.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index(df['date'])\n",
    "#df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ciascun file in cui il dataset è stato tagliato è stato annotato manualmente osservando gli andamenti della\n",
    "#valuta sul grafico in tempo reale del bitcoin.\n",
    "cut = df[:'2019-10-31']\n",
    "cut['value'] = 'bull'\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1 = df['2019-11-01': '2019-11-21']\n",
    "cut1['value'] = 'bear'\n",
    "cut1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut2 = df['2019-10-18': '2019-10-31']\n",
    "cut2['value'] = 'bull'\n",
    "cut2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = df['2019-10-04':]\n",
    "cut3['value'] = 'bull'\n",
    "cut3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut.reset_index(drop=True, inplace=True)\n",
    "cut1.reset_index(drop=True, inplace=True)\n",
    "#cut2.reset_index(drop=True, inplace=True)\n",
    "#cut3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [cut, cut1]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('.../Value10.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop dei duplicati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.../Value10.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=[''], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.../DropDuplicates.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osservazione e visualizzazione dei dati in base a 'bull' e 'bear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['date', 'user','timestamp', 'clean', 'pos', 'neg', 'neu','compound', 'value']\n",
    "df['value'].replace(to_replace=\"bull.+\", value=1, regex=True, inplace=True)\n",
    "df['value'].replace(to_replace=\"bear.+\", value=0, regex=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.value == 'bull']\n",
    "df2 = df[df.value == 'bear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df1['neg']\n",
    "y0 = df1['neu']\n",
    "y2 = df1['pos']\n",
    "trace1 = go.Histogram(\n",
    "    x=y0, name='neu',\n",
    "    opacity=0.75\n",
    ")\n",
    "trace2 = go.Histogram(\n",
    "    x=y1, name = 'neg',\n",
    "    opacity=0.75\n",
    ")\n",
    "trace3 = go.Histogram(\n",
    "    x=y2, name = 'pos',\n",
    "    opacity=0.75\n",
    ")\n",
    "data = [trace1, trace2, trace3]\n",
    "layout = go.Layout(barmode='overlay', title='Distribution')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.pos > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1[df1.neg > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1.compound == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.pos > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.neg > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.compound == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colonna 'week'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Timestamp\n",
    "weeks = [g for n, g in df.set_index('timestamp').groupby(pd.TimeGrouper('W'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df['date'], format = '%Y-%m-%d')\n",
    "df['date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con la colonna 'week' abbiamo annotato ciascun tweet con la settimana dell'anno in cui è stato pubblicato\n",
    "df['week'] = df['date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.../WeekTOT.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizzazione della distribuzione settimanale dei dati\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel(\"week\")\n",
    "plt.ylabel(\"Counts\")\n",
    "df.week.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminazione dei tweets rumorosi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.../Week19.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['clean'].str.contains('bitcoin|btc|crypto|eth|coin|value')]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('.../df2.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero dei tweets per 'bull' e per 'bear' dopo aver filtrato i testi\n",
    "df2['value'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione dataset a frequenza costante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file divisi per settimane\n",
    "df1 = pd.read_csv('.../Week26.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "df2 = pd.read_csv('.../Week28.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenazione fino ad ottenere un file finale train + test\n",
    "df12 = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crazione del train bilanciato\n",
    "df_train=df_final[(df_final['date']!='2019-05-06')&(df_final['date']!='2019-05-11')&(df_final['date']!='2019-06-04')&(df_final['date']!='2019-06-09')&(df_final['date']!='2019-06-26')&(df_final['date']!='2019-06-28')&(df_final['date']!='2019-07-11')&(df_final['date']!='2019-07-12')&(df_final['date']!='2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crazione del test bilanciato\n",
    "df_test=df_final[(df_final['date']=='2019-05-06')|(df_final['date']=='2019-05-11')|(df_final['date']=='2019-06-04')|(df_final['date']=='2019-06-09')|(df_final['date']=='2019-06-26')|(df_final['date']=='2019-06-28')|(df_final['date']=='2019-07-11')|(df_final['date']=='2019-07-12')|(df_final['date']=='2019-07-17')|(df_final['date']=='2019-08-05')|(df_final['date']=='2019-08-09')|(df_final['date']=='2019-08-21')|(df_final['date']=='2019-08-23')|(df_final['date']=='2019-09-10')|(df_final['date']=='2019-09-13')|(df_final['date']=='2019-10-16')|(df_final['date']=='2019-10-20')|(df_final['date']=='2019-10-30')|(df_final['date']=='2019-11-03')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvataggio\n",
    "df_finale.to_csv('.../TrainTestTOT.csv')\n",
    "df_trai.to_csv('.../train.csv')\n",
    "df_test.to_csv('.../test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
