{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4HnTWtmSBD2"
   },
   "source": [
    "# Bert: binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BX_5tmtfSZFx"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLa0OltsR369"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr_7M6TySQCC"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    devicename = \"\"\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__,\n",
    "      'Device:', device, devicename)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1J4daS74g5D"
   },
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.../mean_tfIdf.csv', delimiter=',', skiprows=0, lineterminator='\\n', low_memory=False)\n",
    "#Trasformiamo le etichette bull e bear in valori numerici\n",
    "df['value'].replace(to_replace=\"bull.+\", value=1, regex=True, inplace=True)\n",
    "df['value'].replace(to_replace=\"bear.+\", value=0, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets = df['clean'].values\n",
    "y = df['value'].values\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(tweets, y, test_size=0.3, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3zwOQQ64g5O"
   },
   "outputs": [],
   "source": [
    "x_train = list()\n",
    "y_train = list()\n",
    "\n",
    "for r in x_train1:\n",
    "    x_train.append(r)\n",
    "\n",
    "for r in y_train1:\n",
    "    y_train.append(r)\n",
    "\n",
    "x_test = list()\n",
    "y_test = list()\n",
    "\n",
    "for r in x_test1:\n",
    "    x_test.append(r)\n",
    "\n",
    "for r in y_test1:\n",
    "    y_test.append(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70X8gxqX4g5T"
   },
   "outputs": [],
   "source": [
    "len(x_train),len(y_train),len(x_test),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62X87ZY84g5Z"
   },
   "outputs": [],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r42x4ICg4g5e"
   },
   "outputs": [],
   "source": [
    "sample_idx = 10\n",
    "x_train[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1olVl9d54g5j"
   },
   "outputs": [],
   "source": [
    "y_train[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Sw_15ILDQ8F"
   },
   "source": [
    "# Binary classification\n",
    "\n",
    "Bear vs Bull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQtlnmjUDmvm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_bin = np.asarray(y_train)==y_train[sample_idx]\n",
    "y_test_bin = np.asarray(y_test)==y_train[sample_idx]\n",
    "y_train_bin,y_test_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O3HWztTUmaE"
   },
   "source": [
    "# Prepariamo il testo per BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K46G-KVgUhNQ"
   },
   "outputs": [],
   "source": [
    "train_sentences = [\"[CLS] \" + s for s in x_train]\n",
    "test_sentences = [\"[CLS] \" + s for s in x_test]\n",
    "train_labels = [1 if value else 0 for value in y_train_bin]\n",
    "test_labels = [1 if value else 0 for value in y_test_bin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bcbtpRjhbCN"
   },
   "source": [
    "Adesso convertiamo i tweets in tokens, usando il tokenizer di BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpKdIgJvSD-b"
   },
   "outputs": [],
   "source": [
    "BERTMODEL = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
    "                                          do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXuRy_9Pg3sJ"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_tokenized = [tokenizer.tokenize(s) for s in tqdm(train_sentences)]\n",
    "test_tokenized = [tokenizer.tokenize(s) for s in tqdm(test_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JswOYlyGhfu3"
   },
   "outputs": [],
   "source": [
    "print (\"The full tokenized first training sentence:\")\n",
    "print (train_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAtcXAOzhvMD"
   },
   "outputs": [],
   "source": [
    "TRAIN_MAX_LEN, TEST_MAX_LEN = 128, 512\n",
    "\n",
    "train_tokenized = [t[:(TRAIN_MAX_LEN-1)]+['SEP'] for t in train_tokenized]\n",
    "test_tokenized  = [t[:(TEST_MAX_LEN-1)]+['SEP'] for t in test_tokenized]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (train_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrcrPP8eh_ef"
   },
   "outputs": [],
   "source": [
    "train_ids = [tokenizer.convert_tokens_to_ids(t) for t in train_tokenized]\n",
    "train_ids = np.array([np.pad(i, (0, TRAIN_MAX_LEN-len(i)),\n",
    "                             mode='constant') for i in train_ids])\n",
    "\n",
    "test_ids = [tokenizer.convert_tokens_to_ids(t) for t in test_tokenized]\n",
    "test_ids = np.array([np.pad(i, (0, TEST_MAX_LEN-len(i)),\n",
    "                            mode='constant') for i in test_ids])\n",
    "\n",
    "print (\"The indices of the first training sentence:\")\n",
    "print (ids_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrJ1QioFiQOr"
   },
   "outputs": [],
   "source": [
    "#attention masks\n",
    "amasks_train, amasks_test = [], []\n",
    "\n",
    "for seq in train_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_train.append(seq_mask)\n",
    "\n",
    "for seq in test_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_test.append(seq_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaaKi8FwiU0_"
   },
   "source": [
    "Usiamo train_test_split() di Scikit-Learn per usare il 10% del training dataset come validation set, e poi convertiamo tutti i dati in torch.tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6BDKPqtiSbL"
   },
   "outputs": [],
   "source": [
    "(train_inputs, validation_inputs,\n",
    " train_labels, validation_labels) = train_test_split(train_ids, train_labels,\n",
    "                                                     random_state=42,\n",
    "                                                     test_size=0.1)\n",
    "(train_masks, validation_masks,\n",
    " _, _) = train_test_split(amasks_train, train_ids,\n",
    "                          random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks  = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks  = torch.tensor(validation_masks)\n",
    "test_inputs = torch.tensor(test_ids)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks  = torch.tensor(amasks_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwJkAHmbicwU"
   },
   "source": [
    "Poi creiamo PyTorch *DataLoader* per tutti i set di dati.\n",
    "\n",
    "Abbiamo provato sia 16 che 32 per il batch size. Il risultato era leggermente migliore con 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5wj7YLOiZfZ"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "print('Datasets:')\n",
    "print('Train: ', end=\"\")\n",
    "train_data = TensorDataset(train_inputs, train_masks,\n",
    "                           train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "print(len(train_data), 'tweet')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
    "                                validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\n",
    "                                   sampler=validation_sampler,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "print(len(validation_data), 'tweet')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(len(test_data), 'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEgt4NkqirRi"
   },
   "source": [
    "# BERT MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOG0wOuaiiVr"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(BERTMODEL,\n",
    "                                                      num_labels=2)\n",
    "model.cuda()\n",
    "print('Pretrained BERT model \"{}\" loaded'.format(BERTMODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LElGRFEMUCZS"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naOcRV62iwAR"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR = 2e-5\n",
    "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n",
    "                                 num_training_steps =len(train_dataloader)*EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk4A50LtjIQS"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y617BgqjjAJG"
   },
   "outputs": [],
   "source": [
    "def train(epoch, loss_vector=None, log_interval=200):\n",
    "  # Set model to training mode\n",
    "  model.train()\n",
    "\n",
    "  # Loop over each batch from the training set\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "    # Copy data to GPU if needed\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Zero gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None,\n",
    "                    attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    if loss_vector is not None:\n",
    "        loss_vector.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, step * len(b_input_ids),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * step / len(train_dataloader), loss))\n",
    "\n",
    "def evaluate(loader):\n",
    "  model.eval()\n",
    "\n",
    "  n_correct, n_all = 0, 0\n",
    "\n",
    "  for batch in loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None,\n",
    "                      attention_mask=b_input_mask)\n",
    "      logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    labels = b_labels.to('cpu').numpy()\n",
    "    n_correct += np.sum(predictions == labels)\n",
    "    n_all += len(labels)\n",
    "\n",
    "  print('Accuracy: [{}/{}] {:.4f}'.format(n_correct, n_all,\n",
    "                                          n_correct/n_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtNwgXKDjJQ3"
   },
   "outputs": [],
   "source": [
    "train_lossv = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print()\n",
    "    train(epoch, train_lossv)\n",
    "    print('\\nValidation set:')\n",
    "    evaluate(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmEaoKyxjQQ2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_lossv, label='original')\n",
    "plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101,\n",
    "         label='averaged')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQLpN-Q1jvW7"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDIYn1Mqjz-2"
   },
   "outputs": [],
   "source": [
    "print('Test set:')\n",
    "evaluate(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "11.3 BERT finetune binary.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
